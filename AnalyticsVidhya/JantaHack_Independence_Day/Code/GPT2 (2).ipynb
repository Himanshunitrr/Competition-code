{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B35yoSO6yeYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6C42CDYm61i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/prakhar21/TextAugmentation-GPT2.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bw-1818yYw_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf996066-4e2d-47a0-f030-888007d11bb3"
      },
      "source": [
        "cd TextAugmentation-GPT2/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TextAugmentation-GPT2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtQ42w4yC-_",
        "colab_type": "text"
      },
      "source": [
        "Create train.csv file based on labels.\n",
        "\n",
        "BIO Title+abstract\n",
        "\n",
        "COMP Title+abstract\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnmRfF52sGeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#modified train file\n",
        "#copy paste this in train.py file\n",
        "#I have requested the owner to modify the train.py, hope he accepts my patch\n",
        "'''\n",
        "import csv\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\tdef __init__(self, data_file_name, data_dir='.data/'):\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tdata_path = os.path.join(data_file_name)\n",
        "\n",
        "\t\tself.data_list = []\n",
        "\t\tself.end_of_text_token = \" <|endoftext|> \"\n",
        "\t\t\n",
        "\t\twith open(data_path) as csv_file:\n",
        "\t\t\tcsv_reader = csv.reader(csv_file, delimiter='\\t')\n",
        "\t\t\t\n",
        "\t\t\tfor row in csv_reader:\n",
        "\t\t\t\tdata_str = f\"{row[0]}: {row[1]}{self.end_of_text_token}\"\n",
        "\t\t\t\tself.data_list.append(data_str)\n",
        "\t\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data_list)\n",
        "\n",
        "\tdef __getitem__(self, item):\n",
        "\t\treturn self.data_list[item]\n",
        "\n",
        "def get_data_loader(data_file_name):\n",
        "\tdataset = MyDataset(data_file_name)\n",
        "\tdata_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\treturn data_loader\n",
        "\n",
        "def train(epochs, data_loader, batch_size, tokenizer, model, device):\t\n",
        "\tbatch_counter = 0\n",
        "\tsum_loss = 0.0\n",
        "\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tprint (f'Running {epoch+1} epoch')\n",
        "\n",
        "\t\tfor idx, txt in enumerate(data_loader):\n",
        "\t\t\ttxt = torch.tensor(tokenizer.encode(txt[0]))\n",
        "\t\t\ttxt = txt.unsqueeze(0).to(device)\n",
        "\t\t\toutputs = model(txt, labels=txt)\n",
        "\t\t\tloss, _ = outputs[:2]\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tsum_loss += loss.data\n",
        "\n",
        "\t\t\tif idx%batch_size==0:\n",
        "\t\t\t\tbatch_counter += 1\n",
        "\t\t\t\toptimizer.step()\n",
        "\t\t\t\tscheduler.step()\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tmodel.zero_grad()\n",
        "\n",
        "\t\t\tif batch_counter == 10:\n",
        "\t\t\t\tprint(f\"Total Loss is {sum_loss}\") #printed after every 10*batch_size\n",
        "\t\t\t\tbatch_counter = 0\n",
        "\t\t\t\tsum_loss = 0.0\n",
        "\n",
        "\treturn model\n",
        "\n",
        "def save_model(model, name):\n",
        "\t\"\"\"\n",
        "\tSummary:\n",
        "\t\tSaving model to the Disk\n",
        "\tParameters:\n",
        "\t\tmodel: Trained model object\n",
        "\t\tname: Name of the model to be saved\n",
        "\t\"\"\"\n",
        "\tprint (\"Saving model to Disk\")\n",
        "\ttorch.save(model.state_dict(), f\"{name}.pt\")\n",
        "\treturn\n",
        "\n",
        "def load_models():\n",
        "\t\"\"\"\n",
        "\tSummary:\n",
        "\t\tLoading Pre-trained model\n",
        "\t\"\"\"\n",
        "\tprint ('Loading/Downloading GPT-2 Model')\n",
        "\ttokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "\tmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "\treturn tokenizer, model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\tparser = argparse.ArgumentParser(description='Arguments for training Text Augmentation model')\n",
        "\n",
        "\tparser.add_argument('--epoch', default= 3,type=int, action='store', help='Number of epochs to run')\n",
        "\tparser.add_argument('--warmup', default=300, type=int, action='store', help='Number of warmup steps to run')\n",
        "\tparser.add_argument('--model_name', default='mymodel.pt', type=str, action='store', help='Name of the model file')\n",
        "\tparser.add_argument('--data_file', default='mydata.csv', type=str, action='store', help='Name of the data file')\n",
        "\tparser.add_argument('--batch', type=int, default=32, action='store', help='Batch size')\n",
        "\tparser.add_argument('--learning_rate', default=3e-5, type=float, action='store', help='Learning rate for the model')\n",
        "\tparser.add_argument('--max_len', default=200, type=int, action='store', help='Maximum length of sequence')\n",
        "\targs = parser.parse_args()\n",
        "\n",
        "\tBATCH_SIZE = args.batch\n",
        "\tEPOCHS = args.epoch\n",
        "\tLEARNING_RATE = args.learning_rate\n",
        "\tWARMUP_STEPS = args.warmup\n",
        "\tMAX_SEQ_LEN = args.max_len\n",
        "\tMODEL_NAME = args.model_name\n",
        "\tDATA_FILE = args.data_file\n",
        "\n",
        "\tTOKENIZER, MODEL = load_models()\n",
        "\tLOADER = get_data_loader(DATA_FILE)\n",
        "\n",
        "\tDEVICE = 'cpu'\n",
        "\tif torch.cuda.is_available():\n",
        "\t\tDEVICE = 'cuda'\n",
        "\n",
        "\tmodel = MODEL.to(DEVICE)\n",
        "\tmodel.train()\n",
        "\toptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\tscheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "\n",
        "\tmodel = train(EPOCHS, LOADER, BATCH_SIZE, TOKENIZER, MODEL, DEVICE)\n",
        "\tsave_model(model, MODEL_NAME)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbF0nhk7nJQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 train.py --data_file data/train.csv --epoch 5 --warmup 500 --model_name gpt2 --max_len 500 --learning_rate 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aElHupgvth7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#copy paste this in generate.py file\n",
        "'''\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def choose_from_top_k_top_n(probs, k=50, p=0.8):\n",
        "\tind = np.argpartition(probs, -k)[-k:]\n",
        "\ttop_prob = probs[ind]\n",
        "\ttop_prob = {i: top_prob[idx] for idx,i in enumerate(ind)}\n",
        "\tsorted_top_prob = {k: v for k, v in sorted(top_prob.items(), key=lambda item: item[1], reverse=True)}\n",
        "\t\n",
        "\tt=0\n",
        "\tf=[]\n",
        "\tpr = []\n",
        "\tfor k,v in sorted_top_prob.items():\n",
        "\t  t+=v\n",
        "\t  f.append(k)\n",
        "\t  pr.append(v)\n",
        "\t  if t>=p:\n",
        "\t\t  break\n",
        "\ttop_prob = pr / np.sum(pr)\n",
        "\ttoken_id = np.random.choice(f, 1, p = top_prob)\n",
        "\n",
        "\treturn int(token_id)\n",
        "\n",
        "def generate(tokenizer, model, sentences, label):\n",
        "\twith torch.no_grad():\n",
        "\t  for idx in range(sentences):\n",
        "\t\t  finished = False\n",
        "\t\t  cur_ids = torch.tensor(tokenizer.encode(label)).unsqueeze(0).to('cpu')\n",
        "\t\t  for i in range(100):\n",
        "\t\t\t  outputs = model(cur_ids, labels=cur_ids)\n",
        "\t\t\t  loss, logits = outputs[:2]\n",
        "\n",
        "\t\t\t  softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "\n",
        "\t\t\t  if i < 5:\n",
        "\t\t\t\t  n = 10\n",
        "\t\t\t  else:\n",
        "\t\t\t\t  n = 5\n",
        "\n",
        "\t\t\t  next_token_id = choose_from_top_k_top_n(softmax_logits.to('cpu').numpy()) #top-k-top-n sampling\n",
        "\t\t\t  cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1)\n",
        "\n",
        "\t\t\t  if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
        "\t\t\t\t  finished = True\n",
        "\t\t\t\t  break\n",
        "\n",
        "\t\t  if finished:\t          \n",
        "\t\t\t  output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "\t\t\t  output_text = tokenizer.decode(output_list)\n",
        "\t\t\t  print (output_text)\n",
        "\t\t  else:\n",
        "\t\t\t  output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "\t\t\t  output_text = tokenizer.decode(output_list)\n",
        "\t\t\t  print (output_text)\n",
        "\n",
        "def load_models(model_name):\n",
        "\t\"\"\"\n",
        "\tSummary:\n",
        "\t\tLoading the trained model\n",
        "\t\"\"\"\n",
        "\tprint ('Loading Trained GPT-2 Model')\n",
        "\ttokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "\tmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "\tmodel_path = model_name\n",
        "\tmodel.load_state_dict(torch.load(model_path))\n",
        "\treturn tokenizer, model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\tparser = argparse.ArgumentParser(description='Arguments for inferencing Text Augmentation model')\n",
        "\n",
        "\tparser.add_argument('--model_name', default='mymodel.pt', type=str, action='store', help='Name of the model file')\n",
        "\tparser.add_argument('--sentences', type=int, default=5, action='store', help='Number of sentences in outputs')\n",
        "\tparser.add_argument('--label', type=str, action='store', help='Label for which to produce text')\n",
        "\targs = parser.parse_args()\n",
        "\n",
        "\tSENTENCES = args.sentences\n",
        "\tMODEL_NAME = args.model_name\n",
        "\tLABEL = args.label\n",
        "\n",
        "\tTOKENIZER, MODEL = load_models(MODEL_NAME)\n",
        "\n",
        "\tgenerate(TOKENIZER, MODEL, SENTENCES, LABEL)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2PAg0H6sjpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 generate.py --model_name gpt2 --sentences 10 --label BIO>>out.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}